{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Task** **2**"
      ],
      "metadata": {
        "id": "2B-BSTyTsuhq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "62SOMa0X0N3L"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import mean_absolute_error, r2_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Datsets, Models and tokenizing the input"
      ],
      "metadata": {
        "id": "_R7Ik0pDtRfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_PATH = \"scikit-fingerprints/MoleculeNet_Lipophilicity\"\n",
        "MODEL_NAME = \"ibm/MoLFormer-XL-both-10pct\"\n",
        "EXTERNAL_DATA_PATH = \"External-Dataset_for_Task2.csv\"\n",
        "\n",
        "model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "ext_data = pd.read_csv(EXTERNAL_DATA_PATH)\n",
        "smiles_strings = ext_data['SMILES'].tolist()\n",
        "true_lipophilicity = ext_data['Label'].tolist()\n",
        "\n",
        "inputs = tokenizer(smiles_strings, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "XopB1YKe5hLb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "990e19b9-6792-432c-e389-b5da7bf2acd3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define compute gradient function"
      ],
      "metadata": {
        "id": "zeZgfbfYtlso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(model, inputs, labels):\n",
        "    model.zero_grad()\n",
        "    outputs = model(**inputs).last_hidden_state[:, 0, :]\n",
        "    predictions = outputs.squeeze()\n",
        "    loss = loss_fn(predictions, torch.tensor(labels, dtype=torch.float32))\n",
        "    loss.backward()\n",
        "    gradients = [param.grad.clone().detach() for param in model.parameters() if param.grad is not None]\n",
        "    return gradients"
      ],
      "metadata": {
        "id": "OvxsYeWJ5mJF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing Hessian vector product by using LiSSA approximation"
      ],
      "metadata": {
        "id": "jNhR_FZ9tuhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lissa_approximation(model, inputs, labels, num_iter=10, damp=0.01, scale=25):\n",
        "    v = compute_gradient(model, inputs, labels)\n",
        "    h_estimate = v.copy()\n",
        "\n",
        "    for _ in range(num_iter):\n",
        "        model.zero_grad()\n",
        "        outputs = model(**inputs).last_hidden_state[:, 0, :]\n",
        "        predictions = outputs.squeeze()\n",
        "        loss = loss_fn(predictions, torch.tensor(labels, dtype=torch.float32))\n",
        "        grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
        "        # Compute second-order gradient (Hessian-vector product)\n",
        "        hessian_vector_product = torch.autograd.grad(grads, model.parameters(), grad_outputs=h_estimate, retain_graph=True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            h_estimate = [v_i + (1 - damp) * h_est - hvp / scale for v_i, h_est, hvp in zip(v, h_estimate, hessian_vector_product)] # Update approximation\n",
        "\n",
        "    return h_estimate"
      ],
      "metadata": {
        "id": "mC7jo9qo5pYY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define influence scores function"
      ],
      "metadata": {
        "id": "xyulreziulkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_influence_scores(model, ext_data, tokenizer):\n",
        "    influence_scores = []\n",
        "    for i in tqdm(range(len(ext_data))):\n",
        "        smiles = ext_data.iloc[i]['SMILES']\n",
        "        label = ext_data.iloc[i]['Label']\n",
        "        inputs = tokenizer(smiles, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        h_estimate = lissa_approximation(model, inputs, [label])\n",
        "        influence_score = sum(torch.norm(h).item() for h in h_estimate)\n",
        "        influence_scores.append(influence_score)\n",
        "\n",
        "    ext_data['Influence_Score'] = influence_scores\n",
        "    return ext_data"
      ],
      "metadata": {
        "id": "t2dLSxn75r1n"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute influence scores and save them"
      ],
      "metadata": {
        "id": "OxFCrittu1hv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "influential_data = compute_influence_scores(model, ext_data, tokenizer)\n",
        "top_k = int(0.5 * len(influential_data))  # Selecting top 50%\n",
        "selected_data = influential_data.nlargest(top_k, 'Influence_Score')\n",
        "selected_data.to_csv(\"Selected_High_Impact_Samples.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v86mpnvr8RUC",
        "outputId": "54ce50d6-8003-411d-e376-482254df07b6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([768])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "100%|██████████| 300/300 [37:35<00:00,  7.52s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Training parameters"
      ],
      "metadata": {
        "id": "P_QlouajvR7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EXTERNAL_DATA_PATH = \"Selected_High_Impact_Samples.csv\"\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 8\n",
        "LEARNING_RATE = 5e-5\n",
        "model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "dataset = load_dataset(DATASET_PATH)"
      ],
      "metadata": {
        "id": "p0bCm8u0PJhf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class for handling SMILES strings and targets"
      ],
      "metadata": {
        "id": "vR6uv8Livb5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SMILESDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer, max_length=128):\n",
        "        self.smiles = dataset['SMILES']\n",
        "        self.labels = dataset['label']\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.smiles)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        encoding = self.tokenizer(\n",
        "            self.smiles[i],\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        label = torch.tensor(self.labels[i], dtype=torch.float)\n",
        "        return {key: val.squeeze(0) for key, val in encoding.items()}, label\n"
      ],
      "metadata": {
        "id": "64toq4T66bnJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Dataloaders"
      ],
      "metadata": {
        "id": "9AorSYV8vm_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spliting the Train and Test Sets\n",
        "smiles_list = dataset['train']['SMILES']\n",
        "labels_list = dataset['train']['label']\n",
        "train_smiles, test_smiles, train_labels, test_labels = train_test_split(\n",
        "    smiles_list, labels_list, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Creating the SMILESDataset Objects\n",
        "train_dataset = SMILESDataset({'SMILES': train_smiles, 'label': train_labels}, tokenizer)\n",
        "test_dataset = SMILESDataset({'SMILES': test_smiles, 'label': test_labels}, tokenizer)\n",
        "\n",
        "# Loading the csv of the Selected data\n",
        "selected_data = pd.read_csv(EXTERNAL_DATA_PATH)\n",
        "selected_smiles, selected_labels = selected_data['SMILES'].tolist(), selected_data['Label'].tolist()\n",
        "\n",
        "# Combine the high-impact samples selected with the Lipophilicity training dataset\n",
        "train_smiles.extend(selected_smiles)\n",
        "train_labels.extend(selected_labels)\n",
        "train_dataset = SMILESDataset({'SMILES': train_smiles, 'label': train_labels}, tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "P9H_ruD16hKl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the function to fine-tune the model"
      ],
      "metadata": {
        "id": "iRLtmTM0wcjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FineTuneModel(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super(FineTuneModel, self).__init__()\n",
        "        self.base_model = base_model\n",
        "        self.regressor = nn.Linear(base_model.config.hidden_size, 1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = outputs.last_hidden_state[:, 0, :]\n",
        "        return self.regressor(hidden_state).squeeze()"
      ],
      "metadata": {
        "id": "Yrc5J5X-6nZh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Initialization"
      ],
      "metadata": {
        "id": "1VXfRr_LwkoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tune_model = FineTuneModel(model)\n",
        "fine_tune_model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "optimizer = optim.AdamW(fine_tune_model.parameters(), lr=LEARNING_RATE)\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "h_9hnCi-6q4I"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tune and Evaluate Model"
      ],
      "metadata": {
        "id": "PBXejk1mwzaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, optimizer, loss_fn, epochs):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            inputs, labels = batch\n",
        "            input_ids = inputs[\"input_ids\"].to(device)\n",
        "            attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "            labels = labels.to(device)\n",
        "            predictions = model(input_ids, attention_mask)\n",
        "            loss = loss_fn(predictions, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        metrics = evaluate_model(model, test_loader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}\")\n",
        "        print(f\"MSE: {metrics['MSE']:.4f}\")\n",
        "        print(f\"Mean Absolute Error (MAE): {metrics['MAE']:.4f}\")\n",
        "        print(f\"R² Score: {metrics['R2']:.4f}\")\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "    predictions, actuals = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            inputs, labels = batch\n",
        "            input_ids = inputs[\"input_ids\"].to(device)\n",
        "            attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "            labels = labels.to(device)\n",
        "            preds = model(input_ids, attention_mask).cpu().numpy()\n",
        "            predictions.extend(preds)\n",
        "            actuals.extend(labels.cpu().numpy())\n",
        "\n",
        "    mse = np.mean((np.array(predictions) - np.array(actuals))**2)\n",
        "    mae = mean_absolute_error(actuals, predictions)\n",
        "    r2 = r2_score(actuals, predictions)\n",
        "    return {\"MSE\": mse, \"MAE\": mae, \"R2\": r2}\n",
        "\n",
        "train_model(fine_tune_model, train_loader, optimizer, loss_fn, EPOCHS)"
      ],
      "metadata": {
        "id": "8LDdtD-u6wQI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccffdeed-b8bc-491f-d14a-ef3f415f8736"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8, Loss: 0.9907912941141562\n",
            "MSE: 0.6526\n",
            "Mean Absolute Error (MAE): 0.6333\n",
            "R² Score: 0.5583\n",
            "Epoch 2/8, Loss: 0.48409354835748675\n",
            "MSE: 0.5261\n",
            "Mean Absolute Error (MAE): 0.5691\n",
            "R² Score: 0.6439\n",
            "Epoch 3/8, Loss: 0.3147531156851487\n",
            "MSE: 0.4333\n",
            "Mean Absolute Error (MAE): 0.5032\n",
            "R² Score: 0.7067\n",
            "Epoch 4/8, Loss: 0.22661856358701532\n",
            "MSE: 0.4334\n",
            "Mean Absolute Error (MAE): 0.5071\n",
            "R² Score: 0.7067\n",
            "Epoch 5/8, Loss: 0.17944144833494316\n",
            "MSE: 0.3941\n",
            "Mean Absolute Error (MAE): 0.4753\n",
            "R² Score: 0.7332\n",
            "Epoch 6/8, Loss: 0.14680061656981708\n",
            "MSE: 0.4462\n",
            "Mean Absolute Error (MAE): 0.5089\n",
            "R² Score: 0.6980\n",
            "Epoch 7/8, Loss: 0.1378116269341924\n",
            "MSE: 0.4103\n",
            "Mean Absolute Error (MAE): 0.4858\n",
            "R² Score: 0.7223\n",
            "Epoch 8/8, Loss: 0.10821927062828432\n",
            "MSE: 0.3945\n",
            "Mean Absolute Error (MAE): 0.4772\n",
            "R² Score: 0.7330\n"
          ]
        }
      ]
    }
  ]
}
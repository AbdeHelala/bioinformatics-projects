{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10858853,"sourceType":"datasetVersion","datasetId":6745293}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import wandb\nwandb.login(key=\"890ed8f47f5a7fee963464f6935383bd0cd6ede7\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:22:06.588265Z","iopub.execute_input":"2025-03-10T15:22:06.588559Z","iopub.status.idle":"2025-03-10T15:22:15.055956Z","shell.execute_reply.started":"2025-03-10T15:22:06.588538Z","shell.execute_reply":"2025-03-10T15:22:15.055057Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meslamhelala6\u001b[0m (\u001b[33meslamhelala6-saarland-informatics-campus\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:22:21.860058Z","iopub.execute_input":"2025-03-10T15:22:21.860558Z","iopub.status.idle":"2025-03-10T15:22:21.875886Z","shell.execute_reply.started":"2025-03-10T15:22:21.860528Z","shell.execute_reply":"2025-03-10T15:22:21.874999Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport argparse\nimport datetime\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\nfrom datasets import Dataset\nimport wandb\n\ndataset = pd.read_csv(\"/kaggle/input/external-dataset-for-task2-csv/External-Dataset_for_Task2.csv\")\n\n# Enable CUDA if available, otherwise fallback to CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.cuda.empty_cache()\n\n# Define model and dataset paths\nMODEL_NAME = \"ibm/MoLFormer-XL-both-10pct\"\nDATA_PATH = \"/kaggle/input/external-dataset-for-task2-csv/External-Dataset_for_Task2.csv\"\n\n# Load dataset and handle potential errors\ntry:\n    dataset = pd.read_csv(DATA_PATH)\nexcept Exception as e:\n    print(f\"Error loading dataset: {e}\")\n    exit(1)\n\n# Convert labels to binary using the mean value as a threshold\nthreshold = dataset[\"Label\"].mean()\ndataset[\"Label\"] = (dataset[\"Label\"] >= threshold).astype(int)\n\n# Load tokenizer from the pre-trained model\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n\n# Convert dataset to Hugging Face format and rename the text column\ndataset = Dataset.from_pandas(dataset).rename_columns({\"SMILES\": \"text\"})\n\n# Process labels to match model requirements\ndef preprocess_labels(example):\n    return {\"labels\": int(example[\"Label\"])}\n\ndataset = dataset.map(preprocess_labels)\n\n# Tokenize dataset using max length and truncation settings\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n\ndataset = dataset.map(tokenize_function, batched=True)\ndataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n# Load pre-trained model for sequence classification\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_NAME, num_labels=2, trust_remote_code=True\n).to(device)\n\n# Shuffle and split dataset into training (80%) and evaluation (20%) sets\ndataset = dataset.shuffle(seed=42)\nsplit_idx = int(0.8 * len(dataset))\ntrain_dataset = dataset.select(range(split_idx))\neval_dataset = dataset.select(range(split_idx, len(dataset)))\n\n# Fine-tuning function supporting multiple strategies\ndef fine_tune_model(model, train_dataset, eval_dataset, strategy=\"lora\"):\n    # Ensure WandB does not create duplicate runs\n    if wandb.run is not None:\n        wandb.finish()\n\n    # Initialize WandB for tracking experiments\n    wandb.init(project=\"NNTI-Task3\", name=f\"{strategy}-training\", sync_tensorboard=True)\n\n    # Define training parameters based on strategy\n    num_epochs = 30 if strategy == \"bitfit\" else 20\n    learning_rate = 2e-5 if strategy == \"bitfit\" else 1e-5 if strategy == \"lora\" else 5e-6\n\n    training_args = TrainingArguments(\n        output_dir=f\"/kaggle/working/models/fine_tuned_{strategy}\",\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        logging_steps=10,\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=8,\n        num_train_epochs=num_epochs,\n        learning_rate=learning_rate,\n        weight_decay=0.01,\n        fp16=False,\n        remove_unused_columns=False,\n        report_to=\"wandb\",\n    )\n\n    # Apply fine-tuning strategy\n    if strategy == \"bitfit\":\n        # Enable training only on bias parameters\n        for name, param in model.named_parameters():\n            if \"bias\" not in name:\n                param.requires_grad = False\n\n    elif strategy == \"lora\":\n        from peft import get_peft_model, LoraConfig\n        peft_config = LoraConfig(r=8, lora_alpha=16, target_modules=[\"query\", \"value\"], lora_dropout=0.1)\n        model = get_peft_model(model, peft_config)\n\n    elif strategy == \"ia3\":\n        from peft import get_peft_model, IA3Config\n\n        # Retrieve available module names for debugging\n        available_modules = [name for name, _ in model.named_modules()]\n        print(f\"Available Modules: {available_modules[:20]}\")  # Print first 20 modules for reference\n\n        # Define target and feedforward modules based on model architecture\n        target_modules = [m for m in [\"query\", \"value\", \"dense\", \"dense2\"] if any(m in name for name in available_modules)]\n        feedforward_modules = [m for m in [\"dense\", \"dense2\"] if m in target_modules]  # Feedforward must be subset of target\n\n        if not target_modules:\n            raise ValueError(f\"No valid IA3 target modules found! Available: {available_modules[:20]}\")\n\n        print(f\"IA3 Target Modules: {target_modules}\")\n        print(f\"IA3 Feedforward Modules: {feedforward_modules}\")\n\n        peft_config = IA3Config(target_modules=target_modules, feedforward_modules=feedforward_modules)\n        model = get_peft_model(model, peft_config)\n\n    # Initialize Trainer for model fine-tuning\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n    )\n\n    print(f\"Starting training for {strategy}...\")\n    trainer.train()\n    \n    # Evaluate model performance after training\n    final_metrics = trainer.evaluate()\n    wandb.log(final_metrics)\n    wandb.finish()\n    \n    return model\n\n# Main execution block\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--strategy\", type=str, choices=[\"bitfit\", \"lora\", \"ia3\"], default=\"lora\")\n    args, unknown = parser.parse_known_args()\n\n    # Train model based on selected fine-tuning strategy\n    fine_tuned_model = fine_tune_model(model, train_dataset, eval_dataset, strategy=args.strategy)\n\n    # Save model with a unique timestamp for reproducibility\n    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    save_path = f\"/kaggle/working/models/fine_tuned_{args.strategy}_{timestamp}\"\n\n    os.makedirs(save_path, exist_ok=True)\n    fine_tuned_model.save_pretrained(save_path)\n    tokenizer.save_pretrained(save_path)\n\n    print(f\"Model saved at: {save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:22:25.217744Z","iopub.execute_input":"2025-03-10T15:22:25.218064Z","iopub.status.idle":"2025-03-10T15:23:57.031741Z","shell.execute_reply.started":"2025-03-10T15:22:25.218033Z","shell.execute_reply":"2025-03-10T15:23:57.030970Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ec06f2a3912418ba627c75868aabb07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenization_molformer_fast.py:   0%|          | 0.00/6.50k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0e6fb4959d44f7e8727b51abee16f0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenization_molformer.py:   0%|          | 0.00/9.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c1200b983ba4e1b927280e993afaff3"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ibm/MoLFormer-XL-both-10pct:\n- tokenization_molformer.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/ibm/MoLFormer-XL-both-10pct:\n- tokenization_molformer_fast.py\n- tokenization_molformer.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/41.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"689e4c851ee24e3ea1e023bfe7dd1cab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/54.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8089e56f65c4d48843ef8c07b1546cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bebaf66688434f9c9ebe356f7404dcca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad3b42a2bb354c0eac7c35afc0331f57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39c5121bd8984abdbc11d4435d45400d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9294b04c9ea34d3a9297e5d58aaf9d1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_molformer.py:   0%|          | 0.00/7.60k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6356d31788f144bfab654564434d0135"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ibm/MoLFormer-XL-both-10pct:\n- configuration_molformer.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_molformer.py:   0%|          | 0.00/39.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02312c2378d0400697931b1ec1101eb6"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ibm/MoLFormer-XL-both-10pct:\n- modeling_molformer.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/187M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40b4d3fc90654376aceac335486939c6"}},"metadata":{}},{"name":"stderr","text":"Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250310_152253-lme12fdd</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/eslamhelala6-saarland-informatics-campus/NNTI-Task3/runs/lme12fdd' target=\"_blank\">lora-training</a></strong> to <a href='https://wandb.ai/eslamhelala6-saarland-informatics-campus/NNTI-Task3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/eslamhelala6-saarland-informatics-campus/NNTI-Task3' target=\"_blank\">https://wandb.ai/eslamhelala6-saarland-informatics-campus/NNTI-Task3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/eslamhelala6-saarland-informatics-campus/NNTI-Task3/runs/lme12fdd' target=\"_blank\">https://wandb.ai/eslamhelala6-saarland-informatics-campus/NNTI-Task3/runs/lme12fdd</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"name":"stdout","text":"Starting training for lora...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [600/600 00:53, Epoch 20/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.755400</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.702600</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.720600</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.681400</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.763400</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.763400</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.702800</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.700600</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.712600</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.709700</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.718100</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.679700</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.697400</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.678700</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.695000</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.672800</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.699200</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.693800</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.716600</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.686700</td>\n      <td>No log</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8/8 00:00]\n    </div>\n    "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–</td></tr><tr><td>eval/runtime</td><td>â–‚â–†â–„â–ƒâ–ƒâ–‚â–â–ƒâ–‚â–‚â–â–‚â–â–‚â–„â–‡â–…â–ˆâ–â–‚â–‚</td></tr><tr><td>eval/samples_per_second</td><td>â–‡â–ƒâ–…â–†â–†â–‡â–ˆâ–†â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–…â–‚â–„â–â–ˆâ–‡â–‡</td></tr><tr><td>eval/steps_per_second</td><td>â–‡â–ƒâ–…â–†â–†â–‡â–ˆâ–†â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–…â–‚â–„â–â–ˆâ–‡â–‡</td></tr><tr><td>eval_runtime</td><td>â–</td></tr><tr><td>eval_samples_per_second</td><td>â–</td></tr><tr><td>eval_steps_per_second</td><td>â–</td></tr><tr><td>train/epoch</td><td>â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–ƒâ–ƒâ–‚â–‚â–ƒâ–â–‚â–â–â–ƒâ–…â–‚â–†â–â–‚â–â–‚â–‚â–‚â–„â–‚â–ˆâ–…â–‚â–…â–‚â–‚â–‚â–…â–ƒâ–‚â–‚â–ˆâ–‚â–‚â–†â–‚â–‚â–„â–‚</td></tr><tr><td>train/learning_rate</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–</td></tr><tr><td>train/loss</td><td>â–‡â–‡â–ƒâ–…â–…â–‡â–†â–‚â–ƒâ–ˆâ–ˆâ–†â–ƒâ–ƒâ–„â–ƒâ–„â–„â–ƒâ–„â–ƒâ–â–…â–ƒâ–…â–ƒâ–„â–ƒâ–ƒâ–ƒâ–‚â–„â–â–„â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>eval/runtime</td><td>0.2421</td></tr><tr><td>eval/samples_per_second</td><td>247.801</td></tr><tr><td>eval/steps_per_second</td><td>33.04</td></tr><tr><td>eval_runtime</td><td>0.2421</td></tr><tr><td>eval_samples_per_second</td><td>247.801</td></tr><tr><td>eval_steps_per_second</td><td>33.04</td></tr><tr><td>total_flos</td><td>162344108851200.0</td></tr><tr><td>train/epoch</td><td>20</td></tr><tr><td>train/global_step</td><td>600</td></tr><tr><td>train/grad_norm</td><td>0.26354</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>0.6867</td></tr><tr><td>train_loss</td><td>0.7044</td></tr><tr><td>train_runtime</td><td>55.0348</td></tr><tr><td>train_samples_per_second</td><td>87.218</td></tr><tr><td>train_steps_per_second</td><td>10.902</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">lora-training</strong> at: <a href='https://wandb.ai/eslamhelala6-saarland-informatics-campus/NNTI-Task3/runs/lme12fdd' target=\"_blank\">https://wandb.ai/eslamhelala6-saarland-informatics-campus/NNTI-Task3/runs/lme12fdd</a><br> View project at: <a href='https://wandb.ai/eslamhelala6-saarland-informatics-campus/NNTI-Task3' target=\"_blank\">https://wandb.ai/eslamhelala6-saarland-informatics-campus/NNTI-Task3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250310_152253-lme12fdd/logs</code>"},"metadata":{}},{"name":"stdout","text":"Model saved at: /kaggle/working/models/fine_tuned_lora_20250310_152356\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport argparse\nimport datetime\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\nfrom datasets import Dataset\nimport wandb\n\n# Enable CUDA if available, otherwise fallback to CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.cuda.empty_cache()\n\n# Define model and dataset paths\nMODEL_NAME = \"ibm/MoLFormer-XL-both-10pct\"\nDATA_PATH = \"/kaggle/input/external-dataset-for-task2-csv/External-Dataset_for_Task2.csv\"\n\n# Load dataset and handle potential errors\ntry:\n    dataset = pd.read_csv(DATA_PATH)\nexcept Exception as e:\n    print(f\"Error loading dataset: {e}\")\n    exit(1)\n\n# Convert labels to binary using the mean value as a threshold\nthreshold = dataset[\"Label\"].mean()\ndataset[\"Label\"] = (dataset[\"Label\"] >= threshold).astype(int)\n\n# Load tokenizer from the pre-trained model\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n\n# Convert dataset to Hugging Face format and rename the text column\ndataset = Dataset.from_pandas(dataset).rename_columns({\"SMILES\": \"text\"})\n\n# Process labels to match model requirements\ndef preprocess_labels(example):\n    return {\"labels\": int(example[\"Label\"])}\n\ndataset = dataset.map(preprocess_labels)\n\n# Tokenize dataset using max length and truncation settings\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n\ndataset = dataset.map(tokenize_function, batched=True)\ndataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n# Load pre-trained model for sequence classification\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_NAME, num_labels=2, trust_remote_code=True\n).to(device)\n\n# Shuffle and split dataset into training (80%) and evaluation (20%) sets\ndataset = dataset.shuffle(seed=42)\nsplit_idx = int(0.8 * len(dataset))\ntrain_dataset = dataset.select(range(split_idx))\neval_dataset = dataset.select(range(split_idx, len(dataset)))\n\n# Fine-tuning function supporting multiple strategies\ndef fine_tune_model(model, train_dataset, eval_dataset, strategy=\"bitfit\"):\n    # Ensure WandB does not create duplicate runs\n    if wandb.run is not None:\n        wandb.finish()\n\n    # Initialize WandB for tracking experiments\n    wandb.init(project=\"NNTI-Task3\", name=f\"{strategy}-training\", sync_tensorboard=True)\n\n    # Define training parameters based on strategy\n    num_epochs = 30 if strategy == \"bitfit\" else 20\n    learning_rate = 2e-5 if strategy == \"bitfit\" else 1e-5 if strategy == \"lora\" else 5e-6\n\n    training_args = TrainingArguments(\n        output_dir=f\"/kaggle/working/models/fine_tuned_{strategy}\",\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        logging_steps=10,\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=8,\n        num_train_epochs=num_epochs,\n        learning_rate=learning_rate,\n        weight_decay=0.01,\n        fp16=False,\n        remove_unused_columns=False,\n        report_to=\"wandb\",\n    )\n\n    # Apply fine-tuning strategy\n    if strategy == \"bitfit\":\n        # Enable training only on bias parameters\n        for name, param in model.named_parameters():\n            if \"bias\" not in name:\n                param.requires_grad = False\n\n    elif strategy == \"lora\":\n        from peft import get_peft_model, LoraConfig\n        peft_config = LoraConfig(r=8, lora_alpha=16, target_modules=[\"query\", \"value\"], lora_dropout=0.1)\n        model = get_peft_model(model, peft_config)\n\n    elif strategy == \"ia3\":\n        from peft import get_peft_model, IA3Config\n\n        # Retrieve available module names for debugging\n        available_modules = [name for name, _ in model.named_modules()]\n        print(f\"Available Modules: {available_modules[:20]}\")  # Print first 20 modules for reference\n\n        # Define target and feedforward modules based on model architecture\n        target_modules = [m for m in [\"query\", \"value\", \"dense\", \"dense2\"] if any(m in name for name in available_modules)]\n        feedforward_modules = [m for m in [\"dense\", \"dense2\"] if m in target_modules]  # Feedforward must be subset of target\n\n        if not target_modules:\n            raise ValueError(f\"No valid IA3 target modules found! Available: {available_modules[:20]}\")\n\n        print(f\"IA3 Target Modules: {target_modules}\")\n        print(f\"IA3 Feedforward Modules: {feedforward_modules}\")\n\n        peft_config = IA3Config(target_modules=target_modules, feedforward_modules=feedforward_modules)\n        model = get_peft_model(model, peft_config)\n\n    # Initialize Trainer for model fine-tuning\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n    )\n\n    print(f\"Starting training for {strategy}...\")\n    trainer.train()\n    \n    # Evaluate model performance after training\n    final_metrics = trainer.evaluate()\n    wandb.log(final_metrics)\n    wandb.finish()\n    \n    return model\n\n# Main execution block\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--strategy\", type=str, choices=[\"bitfit\", \"lora\", \"ia3\"], default=\"bitfit\")\n    args, unknown = parser.parse_known_args()\n\n    # Train model based on selected fine-tuning strategy\n    fine_tuned_model = fine_tune_model(model, train_dataset, eval_dataset, strategy=args.strategy)\n\n    # Save model with a unique timestamp for reproducibility\n    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    save_path = f\"/kaggle/working/models/fine_tuned_{args.strategy}_{timestamp}\"\n\n    os.makedirs(save_path, exist_ok=True)\n    fine_tuned_model.save_pretrained(save_path)\n    tokenizer.save_pretrained(save_path)\n\n    print(f\"Model saved at: {save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:24:26.755473Z","iopub.execute_input":"2025-03-10T15:24:26.755848Z","iopub.status.idle":"2025-03-10T15:25:50.185569Z","shell.execute_reply.started":"2025-03-10T15:24:26.755816Z","shell.execute_reply":"2025-03-10T15:25:50.184590Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"531b8a0b3bed4bb49a98f07656671b23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f45feee89c014d3ab8a994d55d477c5c"}},"metadata":{}},{"name":"stderr","text":"Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250310_152427-jj7zo3ec</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/eslamhelala6-saarland-informatics-campus/NNTI-Task3/runs/jj7zo3ec' target=\"_blank\">bitfit-training</a></strong> to <a href='https://wandb.ai/eslamhelala6-saarland-informatics-campus/NNTI-Task3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/eslamhelala6-saarland-informatics-campus/NNTI-Task3' target=\"_blank\">https://wandb.ai/eslamhelala6-saarland-informatics-campus/NNTI-Task3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/eslamhelala6-saarland-informatics-campus/NNTI-Task3/runs/jj7zo3ec' target=\"_blank\">https://wandb.ai/eslamhelala6-saarland-informatics-campus/NNTI-Task3/runs/jj7zo3ec</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Starting training for bitfit...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [900/900 01:13, Epoch 30/30]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.745100</td>\n      <td>0.713842</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.750000</td>\n      <td>0.695506</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.744700</td>\n      <td>0.689087</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.700100</td>\n      <td>0.680335</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.698100</td>\n      <td>0.669130</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.720100</td>\n      <td>0.669266</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.725000</td>\n      <td>0.672656</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.704800</td>\n      <td>0.670189</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.681700</td>\n      <td>0.674717</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.690900</td>\n      <td>0.664269</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.701100</td>\n      <td>0.657326</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.717500</td>\n      <td>0.657562</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.693200</td>\n      <td>0.665731</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.678200</td>\n      <td>0.666009</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.697500</td>\n      <td>0.664946</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.693800</td>\n      <td>0.659944</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.698900</td>\n      <td>0.662818</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.680900</td>\n      <td>0.666221</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.684600</td>\n      <td>0.667226</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.674700</td>\n      <td>0.659797</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.676600</td>\n      <td>0.664012</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.670500</td>\n      <td>0.663143</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.693900</td>\n      <td>0.656329</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.683500</td>\n      <td>0.661126</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.698400</td>\n      <td>0.659626</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.670600</td>\n      <td>0.659424</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.703900</td>\n      <td>0.656201</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.672300</td>\n      <td>0.662593</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.695500</td>\n      <td>0.661151</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.678400</td>\n      <td>0.658155</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8/8 00:00]\n    </div>\n    "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–</td></tr><tr><td>eval/loss</td><td>â–ˆâ–†â–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–â–â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–â–â–â–‚â–‚â–â–ƒ</td></tr><tr><td>eval/runtime</td><td>â–‚â–â–â–‚â–‚â–‚â–ƒâ–ˆâ–‚â–ƒâ–â–â–â–â–â–â–ƒâ–â–â–‚â–â–‚â–‚â–ƒâ–‚â–â–‚â–â–â–‚â–‚</td></tr><tr><td>eval/samples_per_second</td><td>â–‡â–ˆâ–ˆâ–†â–‡â–‡â–†â–â–‡â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–†â–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–‡</td></tr><tr><td>eval/steps_per_second</td><td>â–‡â–ˆâ–ˆâ–†â–‡â–‡â–†â–â–‡â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–†â–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–‡</td></tr><tr><td>eval_loss</td><td>â–</td></tr><tr><td>eval_runtime</td><td>â–</td></tr><tr><td>eval_samples_per_second</td><td>â–</td></tr><tr><td>eval_steps_per_second</td><td>â–</td></tr><tr><td>train/epoch</td><td>â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–ˆâ–„â–â–…â–…â–‚â–„â–â–â–ƒâ–â–‚â–ƒâ–ƒâ–…â–†â–‡â–â–â–…â–ƒâ–„â–…â–„â–‚â–…â–‚â–â–‚â–â–‚â–‚â–‚â–†â–‚â–„â–„â–„â–‚â–‚</td></tr><tr><td>train/learning_rate</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–</td></tr><tr><td>train/loss</td><td>â–‡â–ˆâ–ˆâ–…â–‡â–„â–…â–„â–ƒâ–…â–ƒâ–„â–„â–‚â–„â–…â–„â–‚â–ƒâ–‚â–„â–ƒâ–ƒâ–…â–ƒâ–ƒâ–ƒâ–‚â–…â–â–‚â–ƒâ–…â–ƒâ–„â–„â–‚â–‚â–ƒâ–„</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>eval/loss</td><td>0.66985</td></tr><tr><td>eval/runtime</td><td>0.2261</td></tr><tr><td>eval/samples_per_second</td><td>265.339</td></tr><tr><td>eval/steps_per_second</td><td>35.378</td></tr><tr><td>eval_loss</td><td>0.66985</td></tr><tr><td>eval_runtime</td><td>0.2261</td></tr><tr><td>eval_samples_per_second</td><td>265.339</td></tr><tr><td>eval_steps_per_second</td><td>35.378</td></tr><tr><td>total_flos</td><td>241885417881600.0</td></tr><tr><td>train/epoch</td><td>30</td></tr><tr><td>train/global_step</td><td>900</td></tr><tr><td>train/grad_norm</td><td>0.34802</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>0.6784</td></tr><tr><td>train_loss</td><td>0.6981</td></tr><tr><td>train_runtime</td><td>73.6005</td></tr><tr><td>train_samples_per_second</td><td>97.825</td></tr><tr><td>train_steps_per_second</td><td>12.228</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">bitfit-training</strong> at: <a href='https://wandb.ai/eslamhelala6-saarland-informatics-campus/NNTI-Task3/runs/jj7zo3ec' target=\"_blank\">https://wandb.ai/eslamhelala6-saarland-informatics-campus/NNTI-Task3/runs/jj7zo3ec</a><br> View project at: <a href='https://wandb.ai/eslamhelala6-saarland-informatics-campus/NNTI-Task3' target=\"_blank\">https://wandb.ai/eslamhelala6-saarland-informatics-campus/NNTI-Task3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250310_152427-jj7zo3ec/logs</code>"},"metadata":{}},{"name":"stdout","text":"Model saved at: /kaggle/working/models/fine_tuned_bitfit_20250310_152549\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport argparse\nimport datetime\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\nfrom datasets import Dataset\nimport wandb\n\n# Enable CUDA if available, otherwise fallback to CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.cuda.empty_cache()\n\n# Define model and dataset paths\nMODEL_NAME = \"ibm/MoLFormer-XL-both-10pct\"\nDATA_PATH = \"/kaggle/input/external-dataset-for-task2-csv/External-Dataset_for_Task2.csv\"\n\n# Load dataset and handle potential errors\ntry:\n    dataset = pd.read_csv(DATA_PATH)\nexcept Exception as e:\n    print(f\"Error loading dataset: {e}\")\n    exit(1)\n\n# Convert labels to binary using the mean value as a threshold\nthreshold = dataset[\"Label\"].mean()\ndataset[\"Label\"] = (dataset[\"Label\"] >= threshold).astype(int)\n\n# Load tokenizer from the pre-trained model\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n\n# Convert dataset to Hugging Face format and rename the text column\ndataset = Dataset.from_pandas(dataset).rename_columns({\"SMILES\": \"text\"})\n\n# Process labels to match model requirements\ndef preprocess_labels(example):\n    return {\"labels\": int(example[\"Label\"])}\n\ndataset = dataset.map(preprocess_labels)\n\n# Tokenize dataset using max length and truncation settings\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n\ndataset = dataset.map(tokenize_function, batched=True)\ndataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n# Load pre-trained model for sequence classification\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_NAME, num_labels=2, trust_remote_code=True\n).to(device)\n\n# Shuffle and split dataset into training (80%) and evaluation (20%) sets\ndataset = dataset.shuffle(seed=42)\nsplit_idx = int(0.8 * len(dataset))\ntrain_dataset = dataset.select(range(split_idx))\neval_dataset = dataset.select(range(split_idx, len(dataset)))\n\n# Fine-tuning function supporting multiple strategies\ndef fine_tune_model(model, train_dataset, eval_dataset, strategy=\"ia3\"):\n    # Ensure WandB does not create duplicate runs\n    if wandb.run is not None:\n        wandb.finish()\n\n    # Initialize WandB for tracking experiments\n    wandb.init(project=\"NNTI-Task3\", name=f\"{strategy}-training\", sync_tensorboard=True)\n\n    # Define training parameters based on strategy\n    num_epochs = 30 if strategy == \"bitfit\" else 20\n    learning_rate = 2e-5 if strategy == \"bitfit\" else 1e-5 if strategy == \"lora\" else 5e-6\n\n    training_args = TrainingArguments(\n        output_dir=f\"/kaggle/working/models/fine_tuned_{strategy}\",\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        logging_steps=10,\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=8,\n        num_train_epochs=num_epochs,\n        learning_rate=learning_rate,\n        weight_decay=0.01,\n        fp16=False,\n        remove_unused_columns=False,\n        report_to=\"wandb\",\n    )\n\n    # Apply fine-tuning strategy\n    if strategy == \"bitfit\":\n        # Enable training only on bias parameters\n        for name, param in model.named_parameters():\n            if \"bias\" not in name:\n                param.requires_grad = False\n\n    elif strategy == \"lora\":\n        from peft import get_peft_model, LoraConfig\n        peft_config = LoraConfig(r=8, lora_alpha=16, target_modules=[\"query\", \"value\"], lora_dropout=0.1)\n        model = get_peft_model(model, peft_config)\n\n    elif strategy == \"ia3\":\n        from peft import get_peft_model, IA3Config\n\n        # Retrieve available module names for debugging\n        available_modules = [name for name, _ in model.named_modules()]\n        print(f\"Available Modules: {available_modules[:20]}\")  # Print first 20 modules for reference\n\n        # Define target and feedforward modules based on model architecture\n        target_modules = [m for m in [\"query\", \"value\", \"dense\", \"dense2\"] if any(m in name for name in available_modules)]\n        feedforward_modules = [m for m in [\"dense\", \"dense2\"] if m in target_modules]  # Feedforward must be subset of target\n\n        if not target_modules:\n            raise ValueError(f\"No valid IA3 target modules found! Available: {available_modules[:20]}\")\n\n        print(f\"IA3 Target Modules: {target_modules}\")\n        print(f\"IA3 Feedforward Modules: {feedforward_modules}\")\n\n        peft_config = IA3Config(target_modules=target_modules, feedforward_modules=feedforward_modules)\n        model = get_peft_model(model, peft_config)\n\n    # Initialize Trainer for model fine-tuning\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n    )\n\n    print(f\"Starting training for {strategy}...\")\n    trainer.train()\n    \n    # Evaluate model performance after training\n    final_metrics = trainer.evaluate()\n    wandb.log(final_metrics)\n    wandb.finish()\n    \n    return model\n\n# Main execution block\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--strategy\", type=str, choices=[\"bitfit\", \"lora\", \"ia3\"], default=\"ia3\")\n    args, unknown = parser.parse_known_args()\n\n    # Train model based on selected fine-tuning strategy\n    fine_tuned_model = fine_tune_model(model, train_dataset, eval_dataset, strategy=args.strategy)\n\n    # Save model with a unique timestamp for reproducibility\n    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    save_path = f\"/kaggle/working/models/fine_tuned_{args.strategy}_{timestamp}\"\n\n    os.makedirs(save_path, exist_ok=True)\n    fine_tuned_model.save_pretrained(save_path)\n    tokenizer.save_pretrained(save_path)\n\n    print(f\"Model saved at: {save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:25:53.489603Z","iopub.execute_input":"2025-03-10T15:25:53.489999Z","iopub.status.idle":"2025-03-10T15:26:56.014938Z","shell.execute_reply.started":"2025-03-10T15:25:53.489959Z","shell.execute_reply":"2025-03-10T15:26:56.014232Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6797697d6f18498a83dad20594ecc0d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9cde12bf5e64ef6abff5833397d7625"}},"metadata":{}},{"name":"stderr","text":"Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250310_152554-lcszchqe</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/eslamhelala6-saarland-informatics-campus/NNTI-Task3/runs/lcszchqe' target=\"_blank\">ia3-training</a></strong> to <a href='https://wandb.ai/eslamhelala6-saarland-informatics-campus/NNTI-Task3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/eslamhelala6-saarland-informatics-campus/NNTI-Task3' target=\"_blank\">https://wandb.ai/eslamhelala6-saarland-informatics-campus/NNTI-Task3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/eslamhelala6-saarland-informatics-campus/NNTI-Task3/runs/lcszchqe' target=\"_blank\">https://wandb.ai/eslamhelala6-saarland-informatics-campus/NNTI-Task3/runs/lcszchqe</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Available Modules: ['', 'molformer', 'molformer.embeddings', 'molformer.embeddings.word_embeddings', 'molformer.embeddings.dropout', 'molformer.encoder', 'molformer.encoder.layer', 'molformer.encoder.layer.0', 'molformer.encoder.layer.0.attention', 'molformer.encoder.layer.0.attention.self', 'molformer.encoder.layer.0.attention.self.query', 'molformer.encoder.layer.0.attention.self.key', 'molformer.encoder.layer.0.attention.self.value', 'molformer.encoder.layer.0.attention.self.rotary_embeddings', 'molformer.encoder.layer.0.attention.self.feature_map', 'molformer.encoder.layer.0.attention.self.feature_map.kernel', 'molformer.encoder.layer.0.attention.output', 'molformer.encoder.layer.0.attention.output.dense', 'molformer.encoder.layer.0.attention.output.LayerNorm', 'molformer.encoder.layer.0.attention.output.dropout']\nIA3 Target Modules: ['query', 'value', 'dense', 'dense2']\nIA3 Feedforward Modules: ['dense', 'dense2']\nStarting training for ia3...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [600/600 00:52, Epoch 20/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.744200</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.708000</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.741400</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.721200</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.726100</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.732600</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.717900</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.716000</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.740700</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.755700</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.732900</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.739800</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.706200</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.737100</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.723200</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.719200</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.757500</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.701500</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.745600</td>\n      <td>No log</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.725100</td>\n      <td>No log</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8/8 00:00]\n    </div>\n    "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–</td></tr><tr><td>eval/runtime</td><td>â–„â–ƒâ–…â–â–…â–ƒâ–‚â–‚â–ƒâ–ˆâ–â–ƒâ–‚â–‡â–‚â–ƒâ–‚â–‚â–‚â–ƒâ–‚</td></tr><tr><td>eval/samples_per_second</td><td>â–…â–†â–„â–ˆâ–„â–†â–‡â–†â–†â–â–ˆâ–…â–‡â–‚â–‡â–†â–‡â–†â–‡â–†â–‡</td></tr><tr><td>eval/steps_per_second</td><td>â–…â–†â–„â–ˆâ–„â–†â–‡â–†â–†â–â–ˆâ–…â–‡â–‚â–‡â–†â–‡â–†â–‡â–†â–‡</td></tr><tr><td>eval_runtime</td><td>â–</td></tr><tr><td>eval_samples_per_second</td><td>â–</td></tr><tr><td>eval_steps_per_second</td><td>â–</td></tr><tr><td>train/epoch</td><td>â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–…â–…â–„â–„â–â–ƒâ–‚â–â–ƒâ–‚â–‚â–†â–â–‚â–‚â–‚â–â–ƒâ–‡â–‚â–…â–ˆâ–…â–â–ƒâ–‚â–‚â–‚â–‚â–„â–‚â–‚â–„â–‚â–…â–ƒâ–â–„â–‚â–‚</td></tr><tr><td>train/learning_rate</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–</td></tr><tr><td>train/loss</td><td>â–‚â–‡â–†â–…â–…â–â–‡â–†â–…â–…â–ƒâ–…â–‡â–ƒâ–ƒâ–ƒâ–„â–†â–†â–ƒâ–‡â–‡â–…â–…â–…â–‡â–†â–ƒâ–„â–‡â–ˆâ–ƒâ–ƒâ–„â–‚â–ˆâ–†â–ƒâ–†â–…</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>eval/runtime</td><td>0.2372</td></tr><tr><td>eval/samples_per_second</td><td>252.937</td></tr><tr><td>eval/steps_per_second</td><td>33.725</td></tr><tr><td>eval_runtime</td><td>0.2372</td></tr><tr><td>eval_samples_per_second</td><td>252.937</td></tr><tr><td>eval_steps_per_second</td><td>33.725</td></tr><tr><td>total_flos</td><td>161432476876800.0</td></tr><tr><td>train/epoch</td><td>20</td></tr><tr><td>train/global_step</td><td>600</td></tr><tr><td>train/grad_norm</td><td>0.08989</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>0.7251</td></tr><tr><td>train_loss</td><td>0.73243</td></tr><tr><td>train_runtime</td><td>52.531</td></tr><tr><td>train_samples_per_second</td><td>91.375</td></tr><tr><td>train_steps_per_second</td><td>11.422</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">ia3-training</strong> at: <a href='https://wandb.ai/eslamhelala6-saarland-informatics-campus/NNTI-Task3/runs/lcszchqe' target=\"_blank\">https://wandb.ai/eslamhelala6-saarland-informatics-campus/NNTI-Task3/runs/lcszchqe</a><br> View project at: <a href='https://wandb.ai/eslamhelala6-saarland-informatics-campus/NNTI-Task3' target=\"_blank\">https://wandb.ai/eslamhelala6-saarland-informatics-campus/NNTI-Task3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250310_152554-lcszchqe/logs</code>"},"metadata":{}},{"name":"stdout","text":"Model saved at: /kaggle/working/models/fine_tuned_ia3_20250310_152655\n","output_type":"stream"}],"execution_count":7}]}